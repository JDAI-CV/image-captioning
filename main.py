import os
import sys
import pprint
import random
import time
import tqdm
import logging
import argparse
import numpy as np

import torch
import torch.nn as nn
import torch.multiprocessing as mp
import torch.distributed as dist

import losses
import models
import datasets
from datasets.radiology_dataset import IUXRAY, MIMICCXR
from datasets.tokenizers import Tokenizer
import lib.utils as utils
from lib.utils import AverageMeter
from optimizer.optimizer import Optimizer, build_optimizer
from evaluation.evaler import Evaler
from scorer.scorer import Scorer
from lib.config import cfg, cfg_from_file
from mlclassifier import GCNClassifier


device = torch.device('cuda')
fw_adj = torch.tensor([
#  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ,
# [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08624227881040383, 0.0, 0.0, 0.0, 0.0, 0.08531678128946102, 0.0, 0.0] ,
# [0.0, 0.0, 0.0, 0.01865074607267221, 0.0, 0.2924299133554616, 0.0, 0.0, 0.21304488410089617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17180192556684684, 0.6418055548125825, 0.0, 0.5855658364897064, 0.0, 0.8458489347533727, 0.9602592859311171, 0.4274547554463511, 0.0, 0.7595885904689661, 0.0] ,
# [0.0, 0.0, 0.01865074607267221, 0.0, 0.0, 0.4009853199098073, 1.5255730949811777, 0.0, 0.08521151259101123, 0.631755218959081, 0.0, 0.752383206747696, 0.0, 0.0, 0.331650626508743, 0.426960806313068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7569183619130871, 0.0] ,
# [0.0, 0.0, 0.0, 0.0, 0.0, 0.5610118144338234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49754224048515705, 0.059920020742461305, 0.12193009552667401, 0.0, 0.6916982549261147, 0.0, 0.028649105523448872, 0.0, 1.38484543548606, 0.0, 0.0, 0.0, 0.6395125017555444] ,
# [0.0, 0.0, 0.2924299133554616, 0.4009853199098073, 0.5610118144338234, 0.0, 1.522720025998771, 0.6039632016294225, 1.1321805681072825, 0.9342837995278563, 1.281557969181883, 0.8673131734216728, 1.2434062032175066, 0.3490255809790961, 0.6754221656115674, 0.4370111421665694, 0.0, 0.0, 0.0, 0.0, 0.40348845012792584, 0.0, 0.0, 0.06928636204125185, 0.0] ,
# [0.0, 0.0, 0.0, 1.5255730949811777, 0.0, 1.522720025998771, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1794995623878415, 0.0, 0.0, 1.535623430834679, 0.0, 0.0, 0.06231769272515846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
# [0.0, 0.0, 0.0, 0.0, 0.0, 0.6039632016294225, 0.0, 0.0, 0.0, 1.5819475025089174, 0.0, 0.3162811291776416, 0.0, 0.5912241758519928, 0.7710172862925886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8175373019274815, 0.0, 0.0, 0.0, 0.0] ,
# [0.0, 0.0, 0.21304488410089617, 0.08521151259101123, 0.0, 1.1321805681072825, 0.0, 0.0, 0.0, 0.9061920646608415, 0.0, 0.7391379799976754, 0.0, 0.0, 1.1938741371126225, 0.0952618484445128, 0.0, 0.0, 0.05704063562431511, 0.0, 0.0, 0.16859312153006234, 0.0, 1.376195693906577, 0.0] ,
# [0.0, 0.0, 0.0, 0.631755218959081, 0.0, 0.9342837995278563, 0.0, 1.5819475025089174, 0.9061920646608415, 0.0, 0.9602592859311171, 1.6911467944739096, 0.3242705192111205, 0.39747392323441544, 0.8649491061267922, 0.50827416218806, 0.0, 0.0, 0.0, 0.0, 0.623787049309904, 0.0, 0.021989647338186796, 0.0, 0.46624078048150774] ,
# [0.0, 0.0, 0.0, 0.0, 0.0, 1.281557969181883, 0.0, 0.0, 0.0, 0.9602592859311171, 0.0, 0.793205201267951, 0.0, 1.068148247942302, 1.535623430834679, 0.0, 0.0, 0.46778280083332285, 0.0, 0.0, 1.294461374017791, 0.0, 0.0, 0.0, 0.6669114759436587] ,
# [0.0, 0.0, 0.0, 0.752383206747696, 0.0, 0.8673131734216728, 2.1794995623878415, 0.3162811291776416, 0.7391379799976754, 1.6911467944739096, 0.793205201267951, 0.0, 0.0, 0.007276287257039512, 1.3910422020235713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23358941333252825, 0.0, 0.0, 0.3693909544915901, 0.29918669581834156] ,
# [0.0, 0.0, 0.0, 0.0, 0.49754224048515705, 1.2434062032175066, 0.0, 0.0, 0.0, 0.3242705192111205, 0.0, 0.0, 0.0, 0.4321594812223054, 0.0, 0.20648748355473706, 0.2166398550187551, 0.0, 0.16826627073453929, 0.0, 0.6584726072977941, 0.0, 0.0, 0.0, 1.4172170703435527] ,
# [0.0, 0.0, 0.0, 0.0, 0.059920020742461305, 0.3490255809790961, 0.0, 0.5912241758519928, 0.0, 0.39747392323441544, 1.068148247942302, 0.007276287257039512, 0.4321594812223054, 0.0, 0.6161631241992449, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1179703724409795, 0.35302216066358155, 0.0, 0.6443340011659412, 0.0] ,
# [0.0, 0.0, 0.17180192556684684, 0.331650626508743, 0.12193009552667401, 0.6754221656115674, 1.535623430834679, 0.7710172862925886, 1.1938741371126225, 0.8649491061267922, 1.535623430834679, 1.3910422020235713, 0.0, 0.6161631241992449, 0.0, 0.5240225191561991, 0.0, 0.0, 0.0, 0.0, 1.0937906785556397, 0.3096717197899676, 0.0, 1.430262915176853, 0.3484577448251243] ,
# [0.0, 0.0, 0.6418055548125825, 0.426960806313068, 0.0, 0.4370111421665694, 0.0, 0.0, 0.0952618484445128, 0.50827416218806, 0.0, 0.0, 0.20648748355473706, 0.0, 0.5240225191561991, 0.0, 0.0, 0.0, 0.0, 0.2272906111845001, 0.5060040136535207, 0.0, 0.3096717197899676, 0.4186620034983729, 0.0] ,
# [0.0, 0.0, 0.0, 0.0, 0.6916982549261147, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2166398550187551, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10158746275990369, 0.029803617870273677, 0.0, 0.0, 0.137502534460031, 0.0, 0.07092804383736119] ,
# [0.0, 0.08624227881040383, 0.5855658364897064, 0.0, 0.0, 0.0, 0.06231769272515846, 0.0, 0.0, 0.0, 0.46778280083332285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1461991767058608, 0.845848934753373, 0.0, 0.0, 0.9958502310338198, 0.0, 0.0] ,
# [0.0, 0.0, 0.0, 0.0, 0.028649105523448872, 0.0, 0.0, 0.0, 0.05704063562431511, 0.0, 0.0, 0.0, 0.16826627073453929, 0.0, 0.0, 0.0, 0.10158746275990369, 0.1461991767058608, 0.0, 0.08964361822629091, 0.0034771927022252134, 0.0, 0.08912895017581536, 0.0, 0.06907447518803858] ,
# [0.0, 0.0, 0.8458489347533727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2272906111845001, 0.029803617870273677, 0.845848934753373, 0.08964361822629091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
# [0.0, 0.0, 0.9602592859311171, 0.0, 1.38484543548606, 0.40348845012792584, 0.0, 0.8175373019274815, 0.0, 0.623787049309904, 1.294461374017791, 0.23358941333252825, 0.6584726072977941, 2.1179703724409795, 1.0937906785556397, 0.5060040136535207, 0.0, 0.0, 0.0034771927022252134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
# [0.0, 0.0, 0.4274547554463511, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16859312153006234, 0.0, 0.0, 0.0, 0.0, 0.35302216066358155, 0.3096717197899676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49199327658392233, 0.6449325692248835] ,
# [0.0, 0.08531678128946102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021989647338186796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3096717197899676, 0.137502534460031, 0.9958502310338198, 0.08912895017581536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
# [0.0, 0.0, 0.7595885904689661, 0.7569183619130871, 0.0, 0.06928636204125185, 0.0, 0.0, 1.376195693906577, 0.0, 0.0, 0.3693909544915901, 0.0, 0.6443340011659412, 1.430262915176853, 0.4186620034983729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49199327658392233, 0.0, 0.0, 0.0] ,
# [0.0, 0.0, 0.0, 0.0, 0.6395125017555444, 0.0, 0.0, 0.0, 0.0, 0.46624078048150774, 0.6669114759436587, 0.29918669581834156, 1.4172170703435527, 0.0, 0.3484577448251243, 0.0, 0.07092804383736119, 0.0, 0.06907447518803858, 0.0, 0.0, 0.6449325692248835, 0.0, 0.0, 0.0] ,


[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ,
[0.0, 0.0, 0.554004673298568, 0.3230466430334976, 0.2631825039749924, 0.7016287008480984, 0.24746163509836083, 0.0010015098042039912, 0.5068736479106769, 0.0, 0.9524974820767607, 0.0, 0.274863833501621, 0.41494699762748494, 0.5840689600939755, 0.0, 0.0, 0.1302442384969287, 0.0, 0.6339873741551628, 0.0, 0.0, 0.0, 0.0, 0.13757450134915059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20484800268752174, 0.0, 0.3451355245988348, 0.0, 0.0, 0.31496065589330335] ,
[0.0, 0.554004673298568, 0.0, 0.0, 0.8777428349239078, 0.48756689514453816, 0.15609090494487807, 0.0, 0.2680360388191779, 0.0, 0.6199516595911282, 0.42238326753620825, 0.11635380051050957, 0.0020601220064393085, 0.6321690244338739, 0.0, 0.0, 0.08334337914718853, 0.5194350922631013, 0.12854873550846002, 0.6928120918404297, 0.0, 0.0, 0.9837299370816517, 0.2314271028213519, 0.5817251164456204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33811183663241207, 0.0, 0.13936476949018967, 0.0, 0.0, 0.07089242056885223] ,
[0.0, 0.3230466430334976, 0.0, 0.0, 0.49497396267096466, 0.6594324804768135, 0.0, 0.706453671957811, 0.4967156785324145, 0.0, 0.8143326399158364, 0.09215690907124767, 1.0617056232382251, 0.1725070158434512, 0.3677521118316441, 0.0, 0.0, 0.0, 0.0, 0.31306543839429407, 1.8437693102858905, 0.0, 0.0, 0.0, 0.12775432598701517, 0.0, 0.1052712068749145, 0.0, 0.0, 0.0, 0.0, 0.037260086847355586, 0.0, 0.7070285247253186, 0.0, 0.0, 0.36112875829496877] ,
[0.0, 0.2631825039749924, 0.8777428349239078, 0.49497396267096466, 0.0, 0.5081730413487658, 0.0, 0.0, 0.4345747745875523, 0.0, 0.9271810641740084, 0.0, 0.43551881399664394, 0.0, 0.32510239740035224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5363277306682555, 0.0, 0.0, 0.35190615068655606, 0.5705232126845626, 0.5633978920587661, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6523482076409699, 0.05734284387314562, 0.029039130518496915, 0.0, 0.0, 0.47600439204660566] ,
[0.0, 0.7016287008480984, 0.48756689514453816, 0.6594324804768135, 0.5081730413487658, 0.0, 0.5879416440743396, 0.7282368105665861, 0.6655288739610595, 0.0, 0.5829843060689561, 1.1084681450499108, 0.23022619090918064, 0.7869908341567744, 0.9237236161240492, 0.0, 0.0, 0.00679805799268029, 0.3508343765662928, 0.7283807004431154, 1.2142749896595786, 0.0, 0.0, 0.21380809279477983, 0.0, 0.0, 0.0, 0.7384514725987071, 0.036197082879009246, 0.024547912417246583, 0.0, 0.0031487780150971255, 0.0, 0.17255665551694127, 0.0, 0.30464217507912805, 0.5800348339999603] ,
[0.0, 0.24746163509836083, 0.15609090494487807, 0.0, 0.0, 0.5879416440743396, 0.0, 0.0, 0.0, 0.0, 0.050047418864644054, 1.1040108892060951, 0.0, 1.0758558236410007, 0.20507481280870768, 0.0, 0.0, 0.20482994095079318, 0.20102931464117388, 0.5609723973768375, 0.5907785254095393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1108353406999546, 2.2722344254125035, 0.6708537856962394, 0.0, 0.0, 0.0, 0.656008131578071, 0.019380062443701017, 0.0] ,
[0.0, 0.0010015098042039912, 0.0, 0.706453671957811, 0.0, 0.7282368105665861, 0.0, 0.0, 0.8928317911567151, 0.0, 0.2412417134011427, 1.1756920438484275, 0.45754054092273994, 0.0, 0.0, 0.07862826830847675, 0.0, 0.08084792186159706, 0.17752298133873087, 0.0, 0.23167676395941764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22902820929658516, 0.0, 0.18822158336999106, 1.3653688666139736, 0.0, 0.0, 0.08262146112756505, 0.8291646636467345, 0.3474221602648512, 0.028679991724856257, 0.15823233737507106] ,
[0.0, 0.5068736479106769, 0.2680360388191779, 0.4967156785324145, 0.4345747745875523, 0.6655288739610595, 0.0, 0.8928317911567151, 0.0, 0.0, 0.5266439565166569, 0.7732181640322486, 0.8796823806455875, 0.04357377094721733, 0.38483053231749587, 0.0, 0.0, 0.01715279715950329, 0.0, 0.1935198098014637, 0.7726446050833446, 0.0, 0.0, 0.01203069298044543, 0.009614236915435854, 0.0, 0.028445852424580496, 0.0, 0.017029382779510584, 0.1773653331099141, 0.0, 0.0, 0.0, 0.7478386613505874, 0.0, 0.0, 0.3988597091752934] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11819146504902717, 0.3285711228697073, 0.0, 0.0, 0.0, 0.0, 0.1284847079738626, 0.3441683893946561, 0.0, 0.0, 0.0, 0.056702571766598944, 0.19444256855484526, 0.0, 0.0, 0.034767887372249416, 0.0, 0.05535863446625411, 0.0, 0.0, 0.2006116936209859, 0.0] ,
[0.0, 0.9524974820767607, 0.6199516595911282, 0.8143326399158364, 0.9271810641740084, 0.5829843060689561, 0.050047418864644054, 0.2412417134011427, 0.5266439565166569, 0.0, 0.0, 0.27606588231803425, 0.2702763655065547, 0.5155464612930855, 0.5345046760872463, 0.0, 0.0, 0.0, 0.0, 0.6619248249548156, 0.11676497651446016, 0.0, 0.0, 0.012298886029973912, 0.36531125789699864, 0.22014333096829503, 0.0, 0.0, 0.1421597490181444, 0.0, 0.0, 0.5023838505056603, 0.007094738878332813, 0.16482707917210404, 0.12422641272658975, 0.0, 0.16806740202956802] ,
[0.0, 0.0, 0.42238326753620825, 0.09215690907124767, 0.0, 1.1084681450499108, 1.1040108892060951, 1.1756920438484275, 0.7732181640322486, 0.0, 0.27606588231803425, 0.0, 0.4571939169349286, 0.44285881406541816, 0.014435627721654973, 0.0, 0.0, 0.0, 0.35645685756909534, 0.3273858184572163, 0.0, 0.0, 0.0, 0.0, 0.22124160687808572, 0.11022312228841263, 0.04830169724837715, 0.0, 0.92753653417296, 0.8948599965191218, 0.15171053363715095, 0.28220534987677603, 0.3001183720922399, 0.5143515680120436, 0.2778525728199598, 0.0, 0.0] ,
[0.0, 0.274863833501621, 0.11635380051050957, 1.0617056232382251, 0.43551881399664394, 0.23022619090918064, 0.0, 0.45754054092273994, 0.8796823806455875, 0.0, 0.2702763655065547, 0.4571939169349286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.448038949914647, 0.0, 0.0, 0.07261956980307784, 0.0, 0.0, 0.07421899390203622, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.594487035114069, 0.0, 0.0, 0.0] ,
[0.0, 0.41494699762748494, 0.0020601220064393085, 0.1725070158434512, 0.0, 0.7869908341567744, 1.0758558236410007, 0.0, 0.04357377094721733, 0.0, 0.5155464612930855, 0.44285881406541816, 0.0, 0.0, 0.9595683261315329, 0.0, 0.0, 0.0, 0.0, 2.4365238654909045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4720717303053189, 0.5727697527229471, 0.0, 0.0, 0.0, 0.9575685889709006, 1.2377447423336991, 0.0, 0.0] ,
[0.0, 0.5840689600939755, 0.6321690244338739, 0.3677521118316441, 0.32510239740035224, 0.9237236161240492, 0.20507481280870768, 0.0, 0.38483053231749587, 0.0, 0.5345046760872463, 0.014435627721654973, 0.0, 0.9595683261315329, 0.0, 0.17363312653927349, 0.19895016785061964, 0.0, 0.0, 1.208717109379973, 0.0, 0.015167532069747917, 0.0, 0.06703478683378344, 0.6177554201058338, 0.4406078996794569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3022525474987056, 0.0884563575795982, 0.0, 1.2708509083599127] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07862826830847675, 0.0, 0.11819146504902717, 0.0, 0.0, 0.0, 0.0, 0.17363312653927349, 0.0, 2.447688346482624, 0.17983209096647654, 0.0, 0.0, 0.0, 0.28338141933933964, 0.0, 0.0, 0.29702177034947524, 0.07753776902134689, 0.007825343616291445, 0.0, 0.0, 0.0, 0.0, 0.0, 1.5708374900647346, 0.0, 0.15510336927054139, 0.3542350803376888, 0.32669031419868527] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3285711228697073, 0.0, 0.0, 0.0, 0.0, 0.19895016785061964, 2.447688346482624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14503213452982808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.139880360686599, 0.0, 0.1439133818303898, 0.0, 0.0] ,
[0.0, 0.1302442384969287, 0.08334337914718853, 0.0, 0.0, 0.00679805799268029, 0.20482994095079318, 0.08084792186159706, 0.01715279715950329, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17983209096647654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4124407401173898, 0.27031693276904833, 0.6962440948684386, 0.01868026565576617, 0.0, 0.058711117532272095, 0.3108414982982819, 0.8917732120346288, 0.5028865181216288, 0.3710258416111474, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.5194350922631013, 0.0, 0.0, 0.3508343765662928, 0.20102931464117388, 0.17752298133873087, 0.0, 0.0, 0.0, 0.35645685756909534, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3095615625659667, 0.0, 0.0, 0.33960729056256184, 0.14426353106038403, 0.48295034078748456, 0.04096791345986367, 0.0, 0.21459856249590378, 0.3394630945304, 0.9426984665379273, 0.22121090818098985, 0.3817453264511766, 0.3146902017201184, 0.2525866980495684, 0.041488578522214464, 0.0] ,
[0.0, 0.6339873741551628, 0.12854873550846002, 0.31306543839429407, 0.0, 0.7283807004431154, 0.5609723973768375, 0.0, 0.1935198098014637, 0.0, 0.6619248249548156, 0.3273858184572163, 0.0, 2.4365238654909045, 1.208717109379973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023645690589139633, 0.15053136759027846, 0.0, 0.24360268371473795, 0.0, 0.8821797270781289, 1.442822630962518, 0.0, 0.21531865324924443] ,
[0.0, 0.0, 0.6928120918404297, 1.8437693102858905, 0.5363277306682555, 1.2142749896595786, 0.5907785254095393, 0.23167676395941764, 0.7726446050833446, 0.0, 0.11676497651446016, 0.0, 1.448038949914647, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3095615625659667, 0.0, 0.0, 0.0, 0.0, 0.33727296191859424, 0.0, 0.11424727258813801, 0.004340630185881583, 0.0, 0.0, 0.17398826794432173, 0.699350130525858, 0.0, 0.0, 0.4114035987596012, 0.0, 0.8603703454658308, 0.6698830923247899] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1284847079738626, 0.0, 0.0, 0.0, 0.0, 0.015167532069747917, 0.28338141933933964, 0.14503213452982808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6128893031292931, 0.0, 0.0, 0.0, 0.3311971887232972, 0.0, 0.221278095246663, 0.7821347726775432, 0.4309320418653175] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3441683893946561, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04939114176166614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.9837299370816517, 0.0, 0.35190615068655606, 0.21380809279477983, 0.0, 0.0, 0.01203069298044543, 0.0, 0.012298886029973912, 0.0, 0.07261956980307784, 0.0, 0.06703478683378344, 0.0, 0.0, 0.4124407401173898, 0.33960729056256184, 0.0, 0.33727296191859424, 0.0, 0.0, 0.0, 0.0, 0.08979034229911677, 0.0, 0.38518142976105196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.13757450134915059, 0.2314271028213519, 0.12775432598701517, 0.5705232126845626, 0.0, 0.0, 0.0, 0.009614236915435854, 0.0, 0.36531125789699864, 0.22124160687808572, 0.0, 0.0, 0.6177554201058338, 0.29702177034947524, 0.0, 0.27031693276904833, 0.14426353106038403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.84531965510853, 0.0, 0.0, 0.0, 0.0, 0.03024606051820196, 0.9015367412440394, 0.4187448648843476, 0.11315910768263383, 1.2343811011663848, 0.0, 0.24145505239298407] ,
[0.0, 0.0, 0.5817251164456204, 0.0, 0.5633978920587661, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22014333096829503, 0.11022312228841263, 0.0, 0.0, 0.4406078996794569, 0.07753776902134689, 0.0, 0.6962440948684386, 0.48295034078748456, 0.0, 0.11424727258813801, 0.0, 0.0, 0.08979034229911677, 2.84531965510853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3231554047178068, 1.2532084879227057, 0.5614893243263129, 0.046137943483739924, 1.540851917651678, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.1052712068749145, 0.0, 0.0, 0.0, 0.22902820929658516, 0.028445852424580496, 0.056702571766598944, 0.0, 0.04830169724837715, 0.07421899390203622, 0.0, 0.0, 0.007825343616291445, 0.0, 0.01868026565576617, 0.04096791345986367, 0.0, 0.004340630185881583, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0564784860967394, 0.12567556923304798, 0.06594334312122997, 0.0, 0.0, 0.06688509334837037, 0.36269590043980027, 0.030319915064814112, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.7384514725987071, 0.0, 0.0, 0.0, 0.19444256855484526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04939114176166614, 0.38518142976105196, 0.0, 0.0, 0.0564784860967394, 0.0, 0.0, 0.0, 0.0, 0.2104088857291676, 0.4561074463243395, 0.0, 0.0, 0.0, 0.6247400008366559] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.036197082879009246, 0.1108353406999546, 0.18822158336999106, 0.017029382779510584, 0.0, 0.1421597490181444, 0.92753653417296, 0.0, 0.4720717303053189, 0.0, 0.0, 0.0, 0.058711117532272095, 0.21459856249590378, 0.023645690589139633, 0.0, 0.6128893031292931, 0.0, 0.0, 0.0, 0.0, 0.12567556923304798, 0.0, 0.0, 0.4309948864530749, 0.0, 0.02542734722301152, 0.0, 0.0, 0.2510742769865065, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.024547912417246583, 2.2722344254125035, 1.3653688666139736, 0.1773653331099141, 0.0, 0.0, 0.8948599965191218, 0.0, 0.5727697527229471, 0.0, 0.0, 0.0, 0.3108414982982819, 0.3394630945304, 0.15053136759027846, 0.17398826794432173, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06594334312122997, 0.0, 0.4309948864530749, 0.0, 0.8692491673212553, 0.0, 0.19435111707051575, 0.266221588915103, 0.676971258598654, 0.20262509195680156, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6708537856962394, 0.0, 0.0, 0.034767887372249416, 0.0, 0.15171053363715095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8917732120346288, 0.9426984665379273, 0.0, 0.699350130525858, 0.0, 0.0, 0.0, 0.03024606051820196, 0.3231554047178068, 0.0, 0.0, 0.0, 0.8692491673212553, 0.0, 1.1605097349751408, 0.3461994713504898, 0.0, 1.5082427073912366, 0.0, 0.0] ,
[0.0, 0.20484800268752174, 0.33811183663241207, 0.037260086847355586, 0.6523482076409699, 0.0031487780150971255, 0.0, 0.0, 0.0, 0.0, 0.5023838505056603, 0.28220534987677603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5028865181216288, 0.22121090818098985, 0.24360268371473795, 0.0, 0.0, 0.0, 0.0, 0.9015367412440394, 1.2532084879227057, 0.0, 0.2104088857291676, 0.02542734722301152, 0.0, 1.1605097349751408, 0.0, 1.681814915277294, 0.0, 0.43925238577431364, 0.04338409257246071, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.05734284387314562, 0.0, 0.0, 0.08262146112756505, 0.0, 0.05535863446625411, 0.007094738878332813, 0.3001183720922399, 0.0, 0.0, 0.0, 1.5708374900647346, 2.139880360686599, 0.3710258416111474, 0.3817453264511766, 0.0, 0.0, 0.3311971887232972, 0.0, 0.0, 0.4187448648843476, 0.5614893243263129, 0.06688509334837037, 0.4561074463243395, 0.0, 0.19435111707051575, 0.3461994713504898, 1.681814915277294, 0.0, 0.2674633965945188, 0.8740258958015569, 0.2474464019557586, 0.0] ,
[0.0, 0.3451355245988348, 0.13936476949018967, 0.7070285247253186, 0.029039130518496915, 0.17255665551694127, 0.0, 0.8291646636467345, 0.7478386613505874, 0.0, 0.16482707917210404, 0.5143515680120436, 0.594487035114069, 0.9575685889709006, 0.3022525474987056, 0.0, 0.0, 0.0, 0.3146902017201184, 0.8821797270781289, 0.4114035987596012, 0.0, 0.0, 0.0, 0.11315910768263383, 0.046137943483739924, 0.36269590043980027, 0.0, 0.0, 0.266221588915103, 0.0, 0.0, 0.2674633965945188, 0.0, 0.42342916886466814, 0.0, 0.33323103097930845] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.656008131578071, 0.3474221602648512, 0.0, 0.0, 0.12422641272658975, 0.2778525728199598, 0.0, 1.2377447423336991, 0.0884563575795982, 0.15510336927054139, 0.1439133818303898, 0.0, 0.2525866980495684, 1.442822630962518, 0.0, 0.221278095246663, 0.0, 0.0, 1.2343811011663848, 1.540851917651678, 0.030319915064814112, 0.0, 0.2510742769865065, 0.676971258598654, 1.5082427073912366, 0.43925238577431364, 0.8740258958015569, 0.42342916886466814, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.30464217507912805, 0.019380062443701017, 0.028679991724856257, 0.0, 0.2006116936209859, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3542350803376888, 0.0, 0.0, 0.041488578522214464, 0.0, 0.8603703454658308, 0.7821347726775432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20262509195680156, 0.0, 0.04338409257246071, 0.2474464019557586, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.31496065589330335, 0.07089242056885223, 0.36112875829496877, 0.47600439204660566, 0.5800348339999603, 0.0, 0.15823233737507106, 0.3988597091752934, 0.0, 0.16806740202956802, 0.0, 0.0, 0.0, 1.2708509083599127, 0.32669031419868527, 0.0, 0.0, 0.0, 0.21531865324924443, 0.6698830923247899, 0.4309320418653175, 0.0, 0.0, 0.24145505239298407, 0.0, 0.0, 0.6247400008366559, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33323103097930845, 0.0, 0.0, 0.0] ,

], dtype=torch.float,device=device)
bw_adj = fw_adj.t()

submodel = GCNClassifier(36, fw_adj, bw_adj) 

state_dict = submodel.state_dict()
state_dict.update({k:v for k, v in torch.load('/content/pretrainedKG/iuxray_gcnclassifier_v1_ones3_t0v1t2_lr1e-6_23050521_e180.pth').items() if k in state_dict})
submodel.load_state_dict(state_dict)


class Trainer(object):
    def __init__(self, args):
        super(Trainer, self).__init__()
        self.args = args
        if cfg.SEED > 0:
            random.seed(cfg.SEED)
            torch.manual_seed(cfg.SEED)
            torch.cuda.manual_seed_all(cfg.SEED)

        self.num_gpus = torch.cuda.device_count()
        self.distributed = self.num_gpus > 1
        if self.distributed:
            torch.cuda.set_device(args.local_rank)
            torch.distributed.init_process_group(
                backend="nccl", init_method="env://"
            )
        self.device = torch.device("cuda")
        # self.device = 'cpu'

        self.rl_stage = False
        self.setup_logging()
        self.setup_dataset()
        self.setup_network()
        self.val_evaler = Evaler(
            datasets.create(name = args.dataset_name,
                image_dir=args.image_dir,
                ann_path=args.ann_path,
                tokenizer=self.tokenizer,
                split='val'
            ),
            tokenizer=self.tokenizer
        )  # TODO
        self.test_evaler = Evaler(
            datasets.create(name = args.dataset_name,
                image_dir=args.image_dir,
                ann_path=args.ann_path,
                tokenizer=self.tokenizer,
                split='test'),
            tokenizer=self.tokenizer
        )  # TODO
        self.scorer = Scorer()

    def setup_logging(self):
        self.logger = logging.getLogger(cfg.LOGGER_NAME)
        self.logger.setLevel(logging.INFO)
        if self.distributed and dist.get_rank() > 0:
            return

        ch = logging.StreamHandler(stream=sys.stdout)
        ch.setLevel(logging.INFO)
        formatter = logging.Formatter("[%(levelname)s: %(asctime)s] %(message)s")
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)

        if not os.path.exists(cfg.ROOT_DIR):
            os.makedirs(cfg.ROOT_DIR)

        fh = logging.FileHandler(os.path.join(cfg.ROOT_DIR, cfg.LOGGER_NAME + '.txt'))
        fh.setLevel(logging.INFO)
        fh.setFormatter(formatter)
        self.logger.addHandler(fh)

        self.logger.info('Training with config:')
        self.logger.info(pprint.pformat(cfg))

    def setup_network(self):
        # model = models.create(cfg.MODEL.TYPE, args)
        model = models.create('XTransformer', args, submodel = submodel)

        if self.distributed:
            # this should be removed if we update BatchNorm stats
            self.model = torch.nn.parallel.DistributedDataParallel(
                model.to(self.device),
                device_ids=[self.args.local_rank],
                output_device=self.args.local_rank,
                broadcast_buffers=False
            )
        else:
            # self.model = torch.nn.DataParallel(model).cuda() # strange
            self.model = model.cuda()  # strange

        if self.args.resume > 0:
            self.model.load_state_dict(
                torch.load(self.snapshot_path("caption_model", self.args.resume),
                           map_location=lambda storage, loc: storage)
            )

        # self.optim = Optimizer(self.model)
        self.optim = build_optimizer(args, model)
        self.xe_criterion = losses.create(cfg.LOSSES.XE_TYPE).cuda()
        self.rl_criterion = losses.create(cfg.LOSSES.RL_TYPE).cuda()

    def setup_dataset(self):
        self.tokenizer = Tokenizer(ann_path=args.ann_path, dataset_name=args.dataset_name)
        self.dataset = datasets.create(name = args.dataset_name,
                image_dir=args.image_dir,
                ann_path=args.ann_path,
                tokenizer=self.tokenizer,
                split='train'
            )
        # self.coco_set = datasets.coco_dataset.CocoDataset(
        #     image_ids_path = cfg.DATA_LOADER.TRAIN_ID,
        #     input_seq = cfg.DATA_LOADER.INPUT_SEQ_PATH,
        #     target_seq = cfg.DATA_LOADER.TARGET_SEQ_PATH,
        #     gv_feat_path = cfg.DATA_LOADER.TRAIN_GV_FEAT,
        #     att_feats_folder = cfg.DATA_LOADER.TRAIN_ATT_FEATS,
        #     seq_per_img = cfg.DATA_LOADER.SEQ_PER_IMG,
        #     max_feat_num = cfg.DATA_LOADER.MAX_FEAT
        # )

    def setup_loader(self, epoch):
        self.training_loader = datasets.data_loader.load_train(
            self.distributed, epoch, self.dataset)

    def eval(self, epoch):
        if (epoch + 1) % cfg.SOLVER.TEST_INTERVAL != 0:
            return None
        if self.distributed and dist.get_rank() > 0:
            return None

        val_res = self.val_evaler(self.model, 'val_' + str(epoch + 1))
        self.logger.info('######## Epoch (VAL)' + str(epoch + 1) + ' ########')
        self.logger.info(str(val_res))

        test_res = self.test_evaler(self.model, 'test_' + str(epoch + 1))
        self.logger.info('######## Epoch (TEST)' + str(epoch + 1) + ' ########')
        self.logger.info(str(test_res))

        val = 0
        for score_type, weight in zip(cfg.SCORER.TYPES, cfg.SCORER.WEIGHTS):
            val -= val_res[score_type] * weight
        return val

    def snapshot_path(self, name, epoch):
        snapshot_folder = os.path.join(cfg.ROOT_DIR, 'snapshot')
        return os.path.join(snapshot_folder, name + "_" + str(epoch) + ".pth")

    def save_model(self, epoch):
        if (epoch + 1) % cfg.SOLVER.SNAPSHOT_ITERS != 0:
            return
        if self.distributed and dist.get_rank() > 0:
            return
        snapshot_folder = os.path.join(cfg.ROOT_DIR, 'snapshot')
        if not os.path.exists(snapshot_folder):
            os.mkdir(snapshot_folder)
        torch.save(self.model.state_dict(), self.snapshot_path("caption_model", epoch + 1))

    def make_kwargs(self, indices, input_seq, target_seq, gv_feat, att_feats, att_mask):
        seq_mask = (input_seq > 0).type(torch.cuda.LongTensor)
        # print(seq_mask)
        seq_mask[:, 0] += 1
        seq_mask_sum = seq_mask.sum(-1)
        max_len = int(seq_mask_sum.max())
        input_seq = input_seq[:, 0:max_len].contiguous()
        target_seq = target_seq[:, 0:max_len].contiguous()

        kwargs = {
            cfg.PARAM.INDICES: indices,
            cfg.PARAM.INPUT_SENT: input_seq,
            cfg.PARAM.TARGET_SENT: target_seq,
            cfg.PARAM.GLOBAL_FEAT: gv_feat,
            cfg.PARAM.ATT_FEATS: att_feats,
            cfg.PARAM.ATT_FEATS_MASK: att_mask
        }
        return kwargs

    def scheduled_sampling(self, epoch):
        if epoch > cfg.TRAIN.SCHEDULED_SAMPLING.START:
            frac = (epoch - cfg.TRAIN.SCHEDULED_SAMPLING.START) // cfg.TRAIN.SCHEDULED_SAMPLING.INC_EVERY
            ss_prob = min(cfg.TRAIN.SCHEDULED_SAMPLING.INC_PROB * frac, cfg.TRAIN.SCHEDULED_SAMPLING.MAX_PROB)
            # self.model.ss_prob = ss_prob

    def display(self, iteration, data_time, batch_time, losses, loss_info):
        if iteration % cfg.SOLVER.DISPLAY != 0:
            return
        if self.distributed and dist.get_rank() > 0:
            return
        info_str = ' (DataTime/BatchTime: {:.3}/{:.3}) losses = {:.5}'.format(data_time.avg, batch_time.avg, losses.avg)
        # self.logger.info('Iteration ' + str(iteration) + info_str + ', lr = ' + str(self.optim.get_lr()))
        for name in sorted(loss_info):
            self.logger.info('  ' + name + ' = ' + str(loss_info[name]))
        data_time.reset()
        batch_time.reset()
        losses.reset()

    def forward(self, kwargs):
        if self.rl_stage == False:
            logit = self.model(**kwargs)
            loss, loss_info = self.xe_criterion(logit, kwargs[cfg.PARAM.TARGET_SENT])
        else:
            ids = kwargs[cfg.PARAM.INDICES]
            gv_feat = kwargs[cfg.PARAM.GLOBAL_FEAT]
            att_feats = kwargs[cfg.PARAM.ATT_FEATS]
            att_mask = kwargs[cfg.PARAM.ATT_FEATS_MASK]
            target_seq = kwargs[cfg.PARAM.TARGET_SENT]

            # max
            kwargs['BEAM_SIZE'] = 1
            kwargs['GREEDY_DECODE'] = True
            kwargs[cfg.PARAM.GLOBAL_FEAT] = gv_feat
            kwargs[cfg.PARAM.ATT_FEATS] = att_feats
            kwargs[cfg.PARAM.ATT_FEATS_MASK] = att_mask

            self.model.eval()
            with torch.no_grad():
                seq_max, logP_max = self.model.module.decode(**kwargs)
            self.model.train()
            rewards_max, rewards_info_max = self.scorer(target_seq, seq_max.data.cpu().numpy().tolist())  # Modified
            rewards_max = utils.expand_numpy(rewards_max)

            ids = utils.expand_numpy(ids)  # to check?
            gv_feat = utils.expand_tensor(gv_feat, cfg.DATA_LOADER.SEQ_PER_IMG)
            att_feats = utils.expand_tensor(att_feats, cfg.DATA_LOADER.SEQ_PER_IMG)
            att_mask = utils.expand_tensor(att_mask, cfg.DATA_LOADER.SEQ_PER_IMG)

            # sample
            kwargs['BEAM_SIZE'] = 1
            kwargs['GREEDY_DECODE'] = False
            kwargs[cfg.PARAM.GLOBAL_FEAT] = gv_feat
            kwargs[cfg.PARAM.ATT_FEATS] = att_feats
            kwargs[cfg.PARAM.ATT_FEATS_MASK] = att_mask

            seq_sample, logP_sample = self.model.module.decode(**kwargs)
            rewards_sample, rewards_info_sample = self.scorer(target_seq,
                                                              seq_sample.data.cpu().numpy().tolist())  # Modified

            rewards = rewards_sample - rewards_max
            rewards = torch.from_numpy(rewards).float().cuda()
            loss = self.rl_criterion(seq_sample, logP_sample, rewards)

            loss_info = {}
            for key in rewards_info_sample:
                loss_info[key + '_sample'] = rewards_info_sample[key]
            for key in rewards_info_max:
                loss_info[key + '_max'] = rewards_info_max[key]

        return loss, loss_info

    def train(self):
        self.model.train()
        self.optim.zero_grad()

        iteration = 0
        for epoch in range(cfg.SOLVER.MAX_EPOCH):
            if epoch == cfg.TRAIN.REINFORCEMENT.START:
                self.rl_stage = True
            self.setup_loader(epoch)

            start = time.time()
            data_time = AverageMeter()
            batch_time = AverageMeter()
            losses = AverageMeter()
            for _, (indices, input_seq, target_seq, gv_feat, att_feats, att_mask) in enumerate(self.training_loader):
                data_time.update(time.time() - start)

                input_seq = input_seq.cuda()
                target_seq = target_seq.cuda()
                gv_feat = gv_feat.cuda()
                att_feats = att_feats.cuda()
                att_mask = att_mask.cuda()
                # att_mask = torch.ones(16,70).cuda()
                # print(att_mask.shape)


                kwargs = self.make_kwargs(indices, input_seq, target_seq, gv_feat, att_feats, att_mask)
                loss, loss_info = self.forward(kwargs)
                loss.backward()
                # utils.clip_gradient(self.optim.optimizer, self.model,
                #                     cfg.SOLVER.GRAD_CLIP_TYPE, cfg.SOLVER.GRAD_CLIP)
                self.optim.step()
                self.optim.zero_grad()
                # self.optim.scheduler_step('Iter')

                batch_time.update(time.time() - start)
                start = time.time()
                losses.update(loss.item())
                self.display(iteration, data_time, batch_time, losses, loss_info)
                iteration += 1

                if self.distributed:
                    dist.barrier()

            self.save_model(epoch)
            val = self.eval(epoch)
            # self.optim.scheduler_step('Epoch', val)
            # self.scheduled_sampling(epoch)

            if self.distributed:
                dist.barrier()


def parse_args():
    """
    Parse input arguments
    """
    parser = argparse.ArgumentParser(description='Image Captioning')
    parser.add_argument('--folder', dest='folder', type=str, default=None)
    parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--resume", type=int, default=-1)
    parser.add_argument('--image_dir', type=str, default='/content/iu_xray_resized/images/',
                        help='the path to the directory containing the data.')
    parser.add_argument('--ann_path', type=str, default='/content/iu_xray_resized/annotation.json',
                        help='the path to the directory containing the data.')
    parser.add_argument('--dataset_name', type=str, default='IUXRAY', choices=['IUXRAY', 'MIMICCXR'],
                        help='the dataset to be used.')

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args()
    return args


if __name__ == '__main__':
    args = parse_args()
    print('Called with args:')
    print(args)

    if args.folder is not None:
        cfg_from_file(os.path.join(args.folder, 'config.yml'))
    cfg.ROOT_DIR = args.folder

    trainer = Trainer(args)
    trainer.train()
